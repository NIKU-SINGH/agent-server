<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Voice Assistant</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
        background-color: #f5f5f5;
      }
      .container {
        display: flex;
        flex-direction: column;
        gap: 20px;
        background-color: white;
        padding: 20px;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
      }
      .controls {
        display: flex;
        gap: 10px;
      }
      button {
        padding: 10px 15px;
        cursor: pointer;
        font-size: 16px;
        border: none;
        border-radius: 5px;
        background-color: #4285f4;
        color: white;
        transition: background-color 0.3s;
      }
      button:hover {
        background-color: #3367d6;
      }
      button:disabled {
        background-color: #cccccc;
        cursor: not-allowed;
      }
      .recording {
        background-color: #ff4d4d;
      }
      .recording:hover {
        background-color: #e60000;
      }
      .conversation {
        display: flex;
        flex-direction: column;
        gap: 15px;
        max-height: 400px;
        overflow-y: auto;
        padding: 10px;
        border: 1px solid #eee;
        border-radius: 5px;
      }
      .message {
        padding: 10px 15px;
        border-radius: 8px;
        max-width: 80%;
        word-wrap: break-word;
      }
      .user-message {
        align-self: flex-end;
        background-color: #e3f2fd;
        border: 1px solid #bbdefb;
      }
      .assistant-message {
        align-self: flex-start;
        background-color: #f1f1f1;
        border: 1px solid #e0e0e0;
      }
      .status {
        font-style: italic;
        color: #666;
        padding: 5px 10px;
        background-color: #f9f9f9;
        border-radius: 5px;
        border-left: 3px solid #4285f4;
      }
      .audio-visualizer {
        height: 60px;
        background-color: #f5f5f5;
        margin: 10px 0;
        position: relative;
        border-radius: 5px;
        overflow: hidden;
      }
      .audio-level {
        height: 100%;
        width: 0;
        background-color: #4caf50;
        transition: width 0.1s ease-in-out;
      }
      .settings {
        margin-top: 20px;
        padding: 15px;
        border: 1px solid #eee;
        border-radius: 5px;
      }
      h1 {
        color: #333;
        margin-bottom: 20px;
      }
      .loading {
        display: inline-block;
        width: 20px;
        height: 20px;
        border: 3px solid rgba(0, 0, 0, 0.2);
        border-radius: 50%;
        border-top-color: #4285f4;
        animation: spin 1s ease-in-out infinite;
        margin-left: 10px;
        vertical-align: middle;
      }
      @keyframes spin {
        to {
          transform: rotate(360deg);
        }
      }
      .audio-player {
        margin-top: 10px;
        width: 100%;
      }
      .hidden {
        display: none;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>Voice Assistant</h1>

      <div class="status" id="status">WebSocket: Disconnected</div>

      <div class="controls">
        <button id="connectBtn">Connect to Server</button>
        <button id="recordBtn" disabled>Start Recording</button>
        <button id="clearBtn" disabled>Clear Conversation</button>
      </div>

      <div class="audio-visualizer">
        <div id="audioLevel" class="audio-level"></div>
      </div>

      <div>
        <h3>Conversation:</h3>
        <div id="conversation" class="conversation"></div>
      </div>

      <!-- Hidden audio element for playing responses -->
      <audio id="responseAudio" class="hidden"></audio>
    </div>

    <script>
      // DOM elements
      const statusEl = document.getElementById("status");
      const connectBtn = document.getElementById("connectBtn");
      const recordBtn = document.getElementById("recordBtn");
      const clearBtn = document.getElementById("clearBtn");
      const conversationEl = document.getElementById("conversation");
      const audioLevelEl = document.getElementById("audioLevel");
      const responseAudio = document.getElementById("responseAudio");

      // WebSocket and audio variables
      let socket;
      let mediaRecorder;
      let audioChunks = [];
      let isRecording = false;
      let audioContext;
      let analyser;
      let dataArray;
      let animationId;
      let isProcessing = false;

      // Connect to WebSocket
      connectBtn.addEventListener("click", connectWebSocket);

      function connectWebSocket() {
        // Change URL according to your server
        const wsUrl = "ws://localhost:8000/ws/assistant";

        statusEl.textContent = "WebSocket: Connecting...";
        socket = new WebSocket(wsUrl);

        socket.onopen = () => {
          statusEl.textContent = "WebSocket: Connected";
          recordBtn.disabled = false;
          clearBtn.disabled = false;
          connectBtn.disabled = true;
        };

        socket.onclose = () => {
          statusEl.textContent = "WebSocket: Disconnected";
          recordBtn.disabled = true;
          clearBtn.disabled = true;
          connectBtn.disabled = false;
          if (isRecording) {
            stopRecording();
          }
        };

        socket.onerror = (error) => {
          console.error("WebSocket error:", error);
          statusEl.textContent = "WebSocket: Error occurred";
        };

        // Client-side WebSocket handler
        let audioQueue = [];
        let isPlaying = false;

        socket.onmessage = async (event) => {
          if (event.data instanceof Blob) {
            // Handle audio chunk
            const audioChunk = event.data;
            audioQueue.push(audioChunk);
            if (!isPlaying) {
              playNextChunk();
            }
          } else {
            // Handle JSON messages
            const message = JSON.parse(event.data);
            switch (message.type) {
              case "audio_chunk_start":
                // Prepare for incoming audio chunks
                break;
              case "audio_chunk_end":
                // Mark the end of the stream
                break;
              // ... handle other message types
            }
          }
        };

        async function playNextChunk() {
          if (audioQueue.length === 0) {
            isPlaying = false;
            return;
          }

          isPlaying = true;
          const chunk = audioQueue.shift();
          const audioBlob = new Blob([chunk], { type: "audio/mp3" });
          const audioUrl = URL.createObjectURL(audioBlob);
          const audio = new Audio(audioUrl);

          audio.onended = () => {
            URL.revokeObjectURL(audioUrl);
            playNextChunk();
          };

          await audio.play();
        }
      }

      function handleJsonResponse(data) {
        console.log("Received data:", data);

        if (data.error) {
          console.error("Server error:", data.error);
          statusEl.textContent = `Error: ${data.error}`;
          return;
        }

        // Handle different message types
        switch (data.type) {
          case "transcription":
            // Show user's transcribed speech
            if (data.text) {
              addMessage("user", data.text);
              isProcessing = true;
              statusEl.textContent = "Processing your request...";
            }
            break;

          case "text_response":
            // Show assistant's text response
            if (data.text) {
              addMessage("assistant", data.text);
              statusEl.textContent = "Playing response...";
            }
            break;

          case "config_updated":
            statusEl.textContent = "Configuration updated";
            break;

          case "history_cleared":
            conversationEl.innerHTML = "";
            statusEl.textContent = "Conversation history cleared";
            break;

          case "error":
            statusEl.textContent = `Error: ${data.message}`;
            console.error("Error from server:", data.message);
            break;

          default:
            console.log("Unhandled message type:", data.type);
        }

        // If we were processing a request and got a text response, we're done processing
        if (data.type === "text_response" && isProcessing) {
          isProcessing = false;
        }
      }

      // Add message to conversation
      function addMessage(sender, text) {
        const messageEl = document.createElement("div");
        messageEl.classList.add("message");
        messageEl.classList.add(
          sender === "user" ? "user-message" : "assistant-message"
        );
        messageEl.textContent = text;

        conversationEl.appendChild(messageEl);

        // Scroll to bottom of conversation
        conversationEl.scrollTop = conversationEl.scrollHeight;
      }

      // Handle recording
      recordBtn.addEventListener("click", toggleRecording);

      // Handle clearing conversation
      clearBtn.addEventListener("click", clearConversation);

      function clearConversation() {
        // Send a clear history request to the server
        if (socket && socket.readyState === WebSocket.OPEN) {
          socket.send(
            JSON.stringify({
              type: "history",
              action: "clear",
            })
          );
        } else {
          // Just clear the UI if websocket isn't available
          conversationEl.innerHTML = "";
          statusEl.textContent = "Conversation cleared (locally only)";
        }
      }

      async function toggleRecording() {
        if (!isRecording) {
          startRecording();
        } else {
          stopRecording();
        }
      }

      async function startRecording() {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
            },
          });

          // Set up audio context for visualization
          setupAudioVisualization(stream);

          // Configure media recorder
          const options = { mimeType: "audio/webm" };
          mediaRecorder = new MediaRecorder(stream, options);
          audioChunks = [];

          // Collect audio chunks
          mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
              audioChunks.push(event.data);
            }
          };

          // Handle stop event
          mediaRecorder.onstop = async () => {
            const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
            sendAudioToServer(audioBlob);
          };

          // Start recording with 3-second intervals
          // This will send audio chunks to the server every 3 seconds
          mediaRecorder.start(3000);

          isRecording = true;
          recordBtn.textContent = "Stop Recording";
          recordBtn.classList.add("recording");
          statusEl.textContent = "Listening...";
        } catch (err) {
          console.error("Error accessing microphone:", err);
          alert("Error accessing microphone: " + err.message);
        }
      }

      function stopRecording() {
        if (mediaRecorder && isRecording) {
          mediaRecorder.stop();
          mediaRecorder.stream.getTracks().forEach((track) => track.stop());

          isRecording = false;
          recordBtn.textContent = "Start Recording";
          recordBtn.classList.remove("recording");
          statusEl.textContent = "Processing final audio chunk...";

          // Stop visualization
          if (animationId) {
            cancelAnimationFrame(animationId);
            animationId = null;
          }

          if (audioContext) {
            audioContext.close();
            audioContext = null;
          }

          // Reset visualizer
          audioLevelEl.style.width = "0%";
        }
      }

      // Send audio to server via WebSocket
      function sendAudioToServer(audioBlob) {
        if (socket && socket.readyState === WebSocket.OPEN) {
          socket.send(audioBlob);
          statusEl.textContent = isRecording
            ? "Listening and processing..."
            : "Processing audio...";
        } else {
          alert("WebSocket is not connected");
          stopRecording();
        }
      }

      // Set up audio visualization
      function setupAudioVisualization(stream) {
        // Create audio context
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const source = audioContext.createMediaStreamSource(stream);

        // Create analyser
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;
        source.connect(analyser);

        // Get frequency data
        dataArray = new Uint8Array(analyser.frequencyBinCount);

        // Start visualization
        visualize();
      }

      // Visualize audio
      function visualize() {
        if (!analyser) return;

        analyser.getByteFrequencyData(dataArray);

        // Calculate average level
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) {
          sum += dataArray[i];
        }

        const average = sum / dataArray.length;
        const level = Math.min(100, Math.max(0, average * 2)); // Scale to 0-100%

        // Update visualization
        audioLevelEl.style.width = `${level}%`;

        // Continue animation
        animationId = requestAnimationFrame(visualize);
      }

      // Handle page unload
      window.addEventListener("beforeunload", () => {
        if (socket && socket.readyState === WebSocket.OPEN) {
          socket.close();
        }

        if (isRecording) {
          stopRecording();
        }
      });

      // Allow sending text directly
      // (For testing or when microphone isn't available)
      function sendTextInput(text) {
        if (socket && socket.readyState === WebSocket.OPEN) {
          socket.send(
            JSON.stringify({
              type: "text_input",
              text: text,
              tts: true,
            })
          );
          addMessage("user", text);
          statusEl.textContent = "Processing your message...";
        } else {
          alert("WebSocket is not connected");
        }
      }
    </script>
  </body>
</html>
